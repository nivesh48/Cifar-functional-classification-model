{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_functional.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnNq5McFYXh5SQRCGMj+9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nivesh48/Cifar-functional-classification-model/blob/master/cifar_functional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehftGHxfx83s",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Puml3VU_x8lp",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary Pkgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcX53yt3utHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "0392db46-5ad8-4b85-8249-e21739d326c8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP4qwwzayKh5",
        "colab_type": "text"
      },
      "source": [
        "Loading Training and Testing Data,\n",
        "Converting Labels into Categorical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkH3e30C0WBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8baea8e2-ecc5-457f-931e-3f93273c9129"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.cifar10.load_data()\n",
        "y_train=tf.keras.utils.to_categorical(y_train)\n",
        "y_test=tf.keras.utils.to_categorical(y_test)\n",
        "y_test[7]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHLB94fJyZot",
        "colab_type": "text"
      },
      "source": [
        "Normalizing the pixel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYiMMqeo0Vui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test=x_train/255.0,x_test/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_dGh3RymmD",
        "colab_type": "text"
      },
      "source": [
        "Model architecture and Data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zul9132PvKvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lam=1e-4\n",
        "input_img=keras.Input(shape=(32,32,3))\n",
        "conv3_1=layers.Conv2D(16,(3,3),padding='same',kernel_regularizer=tf.keras.regularizers.l2(lam),activation='relu')(input_img)\n",
        "conv5_1=layers.Conv2D(16,(5,5),padding='same',kernel_regularizer=tf.keras.regularizers.l2(lam),activation='relu')(input_img)\n",
        "conc_1=tf.concat([conv3_1,conv5_1],axis=-1)\n",
        "max_pool1=layers.MaxPool2D((2,2))(conc_1)\n",
        "max_pool1=layers.BatchNormalization()(max_pool1)#\n",
        "max_pool1=layers.Dropout(0.2)(max_pool1)#dropout\n",
        "conv3_2=layers.Conv2D(32,(3,3),padding='same',kernel_regularizer=tf.keras.regularizers.l2(lam),activation='relu')(max_pool1)\n",
        "conv5_2=layers.Conv2D(32,(5,5),padding='same',kernel_regularizer=tf.keras.regularizers.l2(lam),activation='relu')(max_pool1)\n",
        "conc_2=tf.concat([conv3_2,conv5_2],axis=-1)\n",
        "max_pool2=layers.MaxPool2D((2,2))(conc_2)\n",
        "max_pool2=layers.BatchNormalization()(max_pool2)#\n",
        "max_pool2=layers.Dropout(0.2)(max_pool2)#\n",
        "conv_128=layers.Conv2D(128,(3,3),padding='same',activation='relu')(max_pool2)\n",
        "max_pool3=layers.MaxPool2D((2,2))(conv_128)\n",
        "max_pool3=layers.BatchNormalization()(max_pool3)#\n",
        "max_pool3=layers.Dropout(0.3)(max_pool3)#\n",
        "conv_256=layers.Conv2D(256,(3,3),padding='same',activation='relu')(max_pool3)\n",
        "max_pool4=layers.MaxPool2D((2,2))(conv_256)\n",
        "max_pool4=layers.BatchNormalization()(max_pool4)#\n",
        "max_pool4=layers.Dropout(0.4)(max_pool4)#\n",
        "flat=layers.Flatten()(max_pool4)\n",
        "out=layers.Dense(10, activation='softmax')(flat)\n",
        "\n",
        "#datagen\n",
        "\n",
        "datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuUpn5jSzzPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5165f9b-fa76-4a41-dd86-3fab1ec6f8c0"
      },
      "source": [
        "model=keras.Model(inputs=input_img, outputs=out, name='cifar_scale')\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"cifar_scale\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   1216        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2 (TensorFlo [(None, 32, 32, 32)] 0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 32)   0           tf_op_layer_concat_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 32)   128         max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 16, 16, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   25632       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3 (TensorFlo [(None, 16, 16, 64)] 0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 64)     0           tf_op_layer_concat_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 64)     256         max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 8, 8, 64)     0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 128)    73856       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 128)    0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 128)    512         max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 4, 4, 128)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 4, 4, 256)    295168      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 2, 2, 256)    0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 2, 2, 256)    1024        max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 2, 2, 256)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1024)         0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         1049600     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           10250       dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,467,338\n",
            "Trainable params: 1,466,378\n",
            "Non-trainable params: 960\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YyFlKcFzEQ4",
        "colab_type": "text"
      },
      "source": [
        "Scheduling Learning rate and Model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHpVt8we0KWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f607db9-9e29-47b6-c0b4-bc8f02bb09df"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "  lrate=0.001\n",
        "  if epoch >75:\n",
        "    lrate=0.0005\n",
        "  if epoch>100:\n",
        "    lrate=0.0003\n",
        "  return lrate\n",
        "\n",
        "bs=32\n",
        "opt=tf.keras.optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,verbose=2)\n",
        "history=model.fit_generator(datagen.flow(x_train,y_train,batch_size=bs),\n",
        "                            steps_per_epoch=x_train.shape[0] //bs,epochs=150,validation_data=(x_test,y_test),callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 1.6330 - acc: 0.4306Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 161us/sample - loss: 1.5861 - acc: 0.4724\n",
            "1562/1562 [==============================] - 34s 22ms/step - loss: 1.6328 - acc: 0.4308 - val_loss: 1.8267 - val_acc: 0.4724\n",
            "Epoch 2/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 1.3284 - acc: 0.5363Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 138us/sample - loss: 1.2343 - acc: 0.5850\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 1.3283 - acc: 0.5363 - val_loss: 1.1319 - val_acc: 0.5850\n",
            "Epoch 3/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 1.2306 - acc: 0.5784Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 138us/sample - loss: 1.0000 - acc: 0.6257\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 1.2303 - acc: 0.5785 - val_loss: 1.1331 - val_acc: 0.6257\n",
            "Epoch 4/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 1.1786 - acc: 0.6010Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 136us/sample - loss: 1.0508 - acc: 0.6434\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 1.1784 - acc: 0.6011 - val_loss: 1.0497 - val_acc: 0.6434\n",
            "Epoch 5/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 1.1314 - acc: 0.6167Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 1.0037 - acc: 0.6872\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 1.1314 - acc: 0.6166 - val_loss: 0.9306 - val_acc: 0.6872\n",
            "Epoch 6/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 1.0981 - acc: 0.6295Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 138us/sample - loss: 0.9352 - acc: 0.6840\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 1.0981 - acc: 0.6295 - val_loss: 0.9932 - val_acc: 0.6840\n",
            "Epoch 7/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 1.0679 - acc: 0.6422Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 153us/sample - loss: 0.8296 - acc: 0.7041\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 1.0676 - acc: 0.6422 - val_loss: 0.9068 - val_acc: 0.7041\n",
            "Epoch 8/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 1.0377 - acc: 0.6542Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.7620 - acc: 0.7065\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 1.0375 - acc: 0.6542 - val_loss: 0.8667 - val_acc: 0.7065\n",
            "Epoch 9/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 1.0080 - acc: 0.6629Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 157us/sample - loss: 1.1978 - acc: 0.6286\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 1.0078 - acc: 0.6630 - val_loss: 1.3361 - val_acc: 0.6286\n",
            "Epoch 10/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9875 - acc: 0.6737Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.7509 - acc: 0.7235\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9876 - acc: 0.6736 - val_loss: 0.8595 - val_acc: 0.7235\n",
            "Epoch 11/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9569 - acc: 0.6808Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 1.1311 - acc: 0.6454\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9570 - acc: 0.6808 - val_loss: 1.0799 - val_acc: 0.6454\n",
            "Epoch 12/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.9235 - acc: 0.6903Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 156us/sample - loss: 0.9330 - acc: 0.6836\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9233 - acc: 0.6903 - val_loss: 1.0626 - val_acc: 0.6836\n",
            "Epoch 13/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9032 - acc: 0.6998Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 1.0382 - acc: 0.6808\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9032 - acc: 0.6998 - val_loss: 1.0839 - val_acc: 0.6808\n",
            "Epoch 14/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7106Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6536 - acc: 0.7435\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8776 - acc: 0.7105 - val_loss: 0.8087 - val_acc: 0.7435\n",
            "Epoch 15/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8650 - acc: 0.7150Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.9106 - acc: 0.7356\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8651 - acc: 0.7150 - val_loss: 0.8218 - val_acc: 0.7356\n",
            "Epoch 16/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.7187Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.9567 - acc: 0.7195\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8547 - acc: 0.7187 - val_loss: 0.8946 - val_acc: 0.7195\n",
            "Epoch 17/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8488 - acc: 0.7213Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 0.7568 - acc: 0.6980\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8489 - acc: 0.7213 - val_loss: 0.9867 - val_acc: 0.6980\n",
            "Epoch 18/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8407 - acc: 0.7245Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.8013 - acc: 0.7359\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8408 - acc: 0.7245 - val_loss: 0.8106 - val_acc: 0.7359\n",
            "Epoch 19/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8365 - acc: 0.7286Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.6942 - acc: 0.7663\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8368 - acc: 0.7285 - val_loss: 0.7191 - val_acc: 0.7663\n",
            "Epoch 20/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7307Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6127 - acc: 0.7718\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8285 - acc: 0.7307 - val_loss: 0.7039 - val_acc: 0.7718\n",
            "Epoch 21/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7322Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.8630 - acc: 0.7233\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8285 - acc: 0.7322 - val_loss: 0.8774 - val_acc: 0.7233\n",
            "Epoch 22/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8260 - acc: 0.7323Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 1.1310 - acc: 0.6687\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8260 - acc: 0.7323 - val_loss: 1.1552 - val_acc: 0.6687\n",
            "Epoch 23/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8301 - acc: 0.7343Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.5961 - acc: 0.7952\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8301 - acc: 0.7342 - val_loss: 0.6453 - val_acc: 0.7952\n",
            "Epoch 24/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8227 - acc: 0.7362Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 0.6762 - acc: 0.7541\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8226 - acc: 0.7363 - val_loss: 0.7959 - val_acc: 0.7541\n",
            "Epoch 25/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.7347Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.6483 - acc: 0.7797\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8189 - acc: 0.7347 - val_loss: 0.6921 - val_acc: 0.7797\n",
            "Epoch 26/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8260 - acc: 0.7354Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 155us/sample - loss: 0.7528 - acc: 0.7373\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8258 - acc: 0.7354 - val_loss: 0.8920 - val_acc: 0.7373\n",
            "Epoch 27/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8266 - acc: 0.7355Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.9237 - acc: 0.6882\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8268 - acc: 0.7355 - val_loss: 1.0137 - val_acc: 0.6882\n",
            "Epoch 28/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8170 - acc: 0.7385Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.5649 - acc: 0.8048\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8170 - acc: 0.7385 - val_loss: 0.6556 - val_acc: 0.8048\n",
            "Epoch 29/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8210 - acc: 0.7404Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.5982 - acc: 0.7893\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8209 - acc: 0.7404 - val_loss: 0.7094 - val_acc: 0.7893\n",
            "Epoch 30/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8279 - acc: 0.7367Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 147us/sample - loss: 0.7322 - acc: 0.7364\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8279 - acc: 0.7367 - val_loss: 0.9031 - val_acc: 0.7364\n",
            "Epoch 31/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8299 - acc: 0.7368Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.6372 - acc: 0.7616\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8304 - acc: 0.7368 - val_loss: 0.7818 - val_acc: 0.7616\n",
            "Epoch 32/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8279 - acc: 0.7387Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 150us/sample - loss: 0.6554 - acc: 0.7804\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8281 - acc: 0.7387 - val_loss: 0.6941 - val_acc: 0.7804\n",
            "Epoch 33/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8340 - acc: 0.7366Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.6559 - acc: 0.7827\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8341 - acc: 0.7365 - val_loss: 0.7109 - val_acc: 0.7827\n",
            "Epoch 34/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8370 - acc: 0.7370Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 140us/sample - loss: 1.0271 - acc: 0.7179\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8373 - acc: 0.7369 - val_loss: 0.9675 - val_acc: 0.7179\n",
            "Epoch 35/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8447 - acc: 0.7352Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.6729 - acc: 0.7613\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8449 - acc: 0.7352 - val_loss: 0.8032 - val_acc: 0.7613\n",
            "Epoch 36/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8410 - acc: 0.7365Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 138us/sample - loss: 0.8698 - acc: 0.7080\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 0.8410 - acc: 0.7364 - val_loss: 0.9638 - val_acc: 0.7080\n",
            "Epoch 37/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8484 - acc: 0.7357Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.7640 - acc: 0.7318\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8486 - acc: 0.7356 - val_loss: 0.8411 - val_acc: 0.7318\n",
            "Epoch 38/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8497 - acc: 0.7367Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.5788 - acc: 0.7994\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8499 - acc: 0.7367 - val_loss: 0.6399 - val_acc: 0.7994\n",
            "Epoch 39/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8539 - acc: 0.7308Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.7412 - acc: 0.7865\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8539 - acc: 0.7308 - val_loss: 0.7568 - val_acc: 0.7865\n",
            "Epoch 40/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8579 - acc: 0.7330Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.6925 - acc: 0.7567\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8577 - acc: 0.7330 - val_loss: 0.8909 - val_acc: 0.7567\n",
            "Epoch 41/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8625 - acc: 0.7313Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 159us/sample - loss: 0.8066 - acc: 0.7286\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8626 - acc: 0.7313 - val_loss: 0.9399 - val_acc: 0.7286\n",
            "Epoch 42/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8591 - acc: 0.7358Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 150us/sample - loss: 0.6731 - acc: 0.7967\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8594 - acc: 0.7358 - val_loss: 0.6674 - val_acc: 0.7967\n",
            "Epoch 43/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8693 - acc: 0.7290Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6486 - acc: 0.7887\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 0.8697 - acc: 0.7289 - val_loss: 0.6923 - val_acc: 0.7887\n",
            "Epoch 44/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8750 - acc: 0.7285Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.7776 - acc: 0.7699\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8751 - acc: 0.7285 - val_loss: 0.7805 - val_acc: 0.7699\n",
            "Epoch 45/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8712 - acc: 0.7305Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.5789 - acc: 0.7908\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8713 - acc: 0.7305 - val_loss: 0.6836 - val_acc: 0.7908\n",
            "Epoch 46/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8801 - acc: 0.7293Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 152us/sample - loss: 0.7776 - acc: 0.7597\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8805 - acc: 0.7292 - val_loss: 0.7979 - val_acc: 0.7597\n",
            "Epoch 47/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8778 - acc: 0.7280Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.8704 - acc: 0.7018\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8779 - acc: 0.7280 - val_loss: 0.9668 - val_acc: 0.7018\n",
            "Epoch 48/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8915 - acc: 0.7253Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 1.0041 - acc: 0.7119\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8913 - acc: 0.7253 - val_loss: 1.0143 - val_acc: 0.7119\n",
            "Epoch 49/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7267Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.6353 - acc: 0.7787\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8835 - acc: 0.7266 - val_loss: 0.7194 - val_acc: 0.7787\n",
            "Epoch 50/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7254Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.7565 - acc: 0.7719\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8923 - acc: 0.7254 - val_loss: 0.7287 - val_acc: 0.7719\n",
            "Epoch 51/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8820 - acc: 0.7270Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.5755 - acc: 0.7708\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8820 - acc: 0.7270 - val_loss: 0.7360 - val_acc: 0.7708\n",
            "Epoch 52/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7263Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 139us/sample - loss: 0.8455 - acc: 0.7493\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8833 - acc: 0.7264 - val_loss: 0.7854 - val_acc: 0.7493\n",
            "Epoch 53/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.8909 - acc: 0.7240Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 1.7055 - acc: 0.6424\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8908 - acc: 0.7239 - val_loss: 1.8791 - val_acc: 0.6424\n",
            "Epoch 54/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9003 - acc: 0.7230Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.8038 - acc: 0.7550\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9003 - acc: 0.7230 - val_loss: 0.7813 - val_acc: 0.7550\n",
            "Epoch 55/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9001 - acc: 0.7223Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.8580 - acc: 0.7298\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8997 - acc: 0.7224 - val_loss: 0.9374 - val_acc: 0.7298\n",
            "Epoch 56/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7238Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 159us/sample - loss: 0.6120 - acc: 0.7945\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8962 - acc: 0.7239 - val_loss: 0.6813 - val_acc: 0.7945\n",
            "Epoch 57/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9037 - acc: 0.7221Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6913 - acc: 0.7876\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9034 - acc: 0.7222 - val_loss: 0.6901 - val_acc: 0.7876\n",
            "Epoch 58/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9011 - acc: 0.7189Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 140us/sample - loss: 0.9564 - acc: 0.6728\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9012 - acc: 0.7188 - val_loss: 1.0073 - val_acc: 0.6728\n",
            "Epoch 59/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9050 - acc: 0.7231Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 150us/sample - loss: 0.8162 - acc: 0.7251\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9052 - acc: 0.7231 - val_loss: 0.8403 - val_acc: 0.7251\n",
            "Epoch 60/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9022 - acc: 0.7215Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.8138 - acc: 0.7701\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9024 - acc: 0.7214 - val_loss: 0.7248 - val_acc: 0.7701\n",
            "Epoch 61/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7222Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 147us/sample - loss: 0.5959 - acc: 0.7947\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9046 - acc: 0.7222 - val_loss: 0.6793 - val_acc: 0.7947\n",
            "Epoch 62/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.9031 - acc: 0.7205Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 1.5744 - acc: 0.5755\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9032 - acc: 0.7205 - val_loss: 1.5904 - val_acc: 0.5755\n",
            "Epoch 63/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9056 - acc: 0.7198Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.6478 - acc: 0.7885\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9055 - acc: 0.7199 - val_loss: 0.6890 - val_acc: 0.7885\n",
            "Epoch 64/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.9008 - acc: 0.7216Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.6743 - acc: 0.7917\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9005 - acc: 0.7217 - val_loss: 0.7032 - val_acc: 0.7917\n",
            "Epoch 65/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9000 - acc: 0.7215Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.8217 - acc: 0.7826\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9002 - acc: 0.7214 - val_loss: 0.7109 - val_acc: 0.7826\n",
            "Epoch 66/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9007 - acc: 0.7223Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.7337 - acc: 0.7464\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.9007 - acc: 0.7224 - val_loss: 0.8682 - val_acc: 0.7464\n",
            "Epoch 67/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9029 - acc: 0.7201Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.7386 - acc: 0.7440\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9030 - acc: 0.7201 - val_loss: 0.7926 - val_acc: 0.7440\n",
            "Epoch 68/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9052 - acc: 0.7196Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.6806 - acc: 0.7840\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9052 - acc: 0.7196 - val_loss: 0.7296 - val_acc: 0.7840\n",
            "Epoch 69/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9019 - acc: 0.7215Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.6401 - acc: 0.7910\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9019 - acc: 0.7214 - val_loss: 0.6982 - val_acc: 0.7910\n",
            "Epoch 70/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8996 - acc: 0.7202Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 147us/sample - loss: 0.7736 - acc: 0.7604\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8996 - acc: 0.7201 - val_loss: 0.7685 - val_acc: 0.7604\n",
            "Epoch 71/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8914 - acc: 0.7225Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 154us/sample - loss: 0.7672 - acc: 0.7520\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8916 - acc: 0.7225 - val_loss: 0.7958 - val_acc: 0.7520\n",
            "Epoch 72/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.7212Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 0.8013 - acc: 0.7801\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9017 - acc: 0.7210 - val_loss: 0.7107 - val_acc: 0.7801\n",
            "Epoch 73/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9002 - acc: 0.7204Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 161us/sample - loss: 0.8380 - acc: 0.7736\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9001 - acc: 0.7205 - val_loss: 0.7422 - val_acc: 0.7736\n",
            "Epoch 74/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.9000 - acc: 0.7218Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.6802 - acc: 0.7881\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8999 - acc: 0.7219 - val_loss: 0.7033 - val_acc: 0.7881\n",
            "Epoch 75/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8952 - acc: 0.7255Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.8581 - acc: 0.7370\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8956 - acc: 0.7255 - val_loss: 0.8227 - val_acc: 0.7370\n",
            "Epoch 76/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8902 - acc: 0.7238Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.7016 - acc: 0.7570\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8903 - acc: 0.7238 - val_loss: 0.7699 - val_acc: 0.7570\n",
            "Epoch 77/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7794 - acc: 0.7573Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 147us/sample - loss: 0.6220 - acc: 0.7980\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7794 - acc: 0.7573 - val_loss: 0.6606 - val_acc: 0.7980\n",
            "Epoch 78/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7632 - acc: 0.7640Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.7020 - acc: 0.7837\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7633 - acc: 0.7639 - val_loss: 0.7078 - val_acc: 0.7837\n",
            "Epoch 79/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7607 - acc: 0.7626Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 154us/sample - loss: 0.6433 - acc: 0.8046\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7609 - acc: 0.7626 - val_loss: 0.6430 - val_acc: 0.8046\n",
            "Epoch 80/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7666 - acc: 0.7622Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 147us/sample - loss: 0.6460 - acc: 0.7985\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7667 - acc: 0.7621 - val_loss: 0.6979 - val_acc: 0.7985\n",
            "Epoch 81/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7578Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.8635 - acc: 0.7783\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7748 - acc: 0.7578 - val_loss: 0.6951 - val_acc: 0.7783\n",
            "Epoch 82/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7705 - acc: 0.7598Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.5339 - acc: 0.8282\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7704 - acc: 0.7598 - val_loss: 0.5620 - val_acc: 0.8282\n",
            "Epoch 83/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7737 - acc: 0.7603Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.5837 - acc: 0.8171\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7734 - acc: 0.7603 - val_loss: 0.6246 - val_acc: 0.8171\n",
            "Epoch 84/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7662 - acc: 0.7615Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 0.6639 - acc: 0.8097\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7659 - acc: 0.7616 - val_loss: 0.6151 - val_acc: 0.8097\n",
            "Epoch 85/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7725 - acc: 0.7581Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 153us/sample - loss: 0.5548 - acc: 0.8019\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7725 - acc: 0.7580 - val_loss: 0.6322 - val_acc: 0.8019\n",
            "Epoch 86/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7725 - acc: 0.7592Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.5924 - acc: 0.8020\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7728 - acc: 0.7593 - val_loss: 0.6243 - val_acc: 0.8020\n",
            "Epoch 87/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7687 - acc: 0.7611Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.7711 - acc: 0.7673\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7686 - acc: 0.7611 - val_loss: 0.7629 - val_acc: 0.7673\n",
            "Epoch 88/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7774 - acc: 0.7594Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 158us/sample - loss: 0.5412 - acc: 0.8088\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7770 - acc: 0.7595 - val_loss: 0.6333 - val_acc: 0.8088\n",
            "Epoch 89/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7757 - acc: 0.7590Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.5771 - acc: 0.8035\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7755 - acc: 0.7590 - val_loss: 0.6507 - val_acc: 0.8035\n",
            "Epoch 90/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7766 - acc: 0.7585Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.5798 - acc: 0.8175\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7762 - acc: 0.7586 - val_loss: 0.5975 - val_acc: 0.8175\n",
            "Epoch 91/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7734 - acc: 0.7585Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 147us/sample - loss: 0.6433 - acc: 0.7804\n",
            "1562/1562 [==============================] - 34s 22ms/step - loss: 0.7733 - acc: 0.7586 - val_loss: 0.6968 - val_acc: 0.7804\n",
            "Epoch 92/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7756 - acc: 0.7592Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.6398 - acc: 0.8016\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7755 - acc: 0.7593 - val_loss: 0.6285 - val_acc: 0.8016\n",
            "Epoch 93/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7700 - acc: 0.7593Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.6067 - acc: 0.8119\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7700 - acc: 0.7592 - val_loss: 0.6322 - val_acc: 0.8119\n",
            "Epoch 94/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7755 - acc: 0.7589Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 154us/sample - loss: 0.8635 - acc: 0.7616\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7758 - acc: 0.7588 - val_loss: 0.7345 - val_acc: 0.7616\n",
            "Epoch 95/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7767 - acc: 0.7557Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 0.9551 - acc: 0.7029\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7766 - acc: 0.7557 - val_loss: 0.8928 - val_acc: 0.7029\n",
            "Epoch 96/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7747 - acc: 0.7563Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6669 - acc: 0.7604\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7750 - acc: 0.7562 - val_loss: 0.7462 - val_acc: 0.7604\n",
            "Epoch 97/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7573Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.5803 - acc: 0.8073\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7759 - acc: 0.7573 - val_loss: 0.6177 - val_acc: 0.8073\n",
            "Epoch 98/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7792 - acc: 0.7578Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.6599 - acc: 0.8111\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7792 - acc: 0.7578 - val_loss: 0.5996 - val_acc: 0.8111\n",
            "Epoch 99/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7698 - acc: 0.7577Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.7603 - acc: 0.7921\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7702 - acc: 0.7576 - val_loss: 0.6645 - val_acc: 0.7921\n",
            "Epoch 100/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7703 - acc: 0.7590Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.6148 - acc: 0.7993\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7701 - acc: 0.7591 - val_loss: 0.6729 - val_acc: 0.7993\n",
            "Epoch 101/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7587Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.7715 - acc: 0.7583\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7712 - acc: 0.7587 - val_loss: 0.7547 - val_acc: 0.7583\n",
            "Epoch 102/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7030 - acc: 0.7785Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.5297 - acc: 0.8314\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7030 - acc: 0.7785 - val_loss: 0.5452 - val_acc: 0.8314\n",
            "Epoch 103/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7064 - acc: 0.7786Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 155us/sample - loss: 0.6827 - acc: 0.7944\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7065 - acc: 0.7786 - val_loss: 0.6602 - val_acc: 0.7944\n",
            "Epoch 104/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7015 - acc: 0.7780Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6363 - acc: 0.8086\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7017 - acc: 0.7779 - val_loss: 0.6080 - val_acc: 0.8086\n",
            "Epoch 105/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7088 - acc: 0.7770Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.5880 - acc: 0.8014\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7088 - acc: 0.7769 - val_loss: 0.6274 - val_acc: 0.8014\n",
            "Epoch 106/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7098 - acc: 0.7771Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.5843 - acc: 0.8092\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7098 - acc: 0.7771 - val_loss: 0.6111 - val_acc: 0.8092\n",
            "Epoch 107/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7040 - acc: 0.7810Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.5647 - acc: 0.8190\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7042 - acc: 0.7809 - val_loss: 0.5850 - val_acc: 0.8190\n",
            "Epoch 108/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6999 - acc: 0.7776Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.5429 - acc: 0.8276\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6998 - acc: 0.7777 - val_loss: 0.6429 - val_acc: 0.8276\n",
            "Epoch 109/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7108 - acc: 0.7780Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.7071 - acc: 0.7610\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7109 - acc: 0.7780 - val_loss: 0.7882 - val_acc: 0.7610\n",
            "Epoch 110/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7074 - acc: 0.7781Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 147us/sample - loss: 0.5134 - acc: 0.8217\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7073 - acc: 0.7782 - val_loss: 0.6190 - val_acc: 0.8217\n",
            "Epoch 111/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7081 - acc: 0.7778Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.6641 - acc: 0.8066\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 0.7081 - acc: 0.7777 - val_loss: 0.6329 - val_acc: 0.8066\n",
            "Epoch 112/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7115 - acc: 0.7765Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.7568 - acc: 0.7896\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7119 - acc: 0.7763 - val_loss: 0.6533 - val_acc: 0.7896\n",
            "Epoch 113/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7119 - acc: 0.7760Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.5447 - acc: 0.8204\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7119 - acc: 0.7759 - val_loss: 0.5750 - val_acc: 0.8204\n",
            "Epoch 114/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7093 - acc: 0.7788Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 0.6108 - acc: 0.8182\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 0.7094 - acc: 0.7787 - val_loss: 0.6002 - val_acc: 0.8182\n",
            "Epoch 115/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7102 - acc: 0.7781Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.6074 - acc: 0.8238\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7101 - acc: 0.7781 - val_loss: 0.5785 - val_acc: 0.8238\n",
            "Epoch 116/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7197 - acc: 0.7729Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.5982 - acc: 0.8114\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7194 - acc: 0.7729 - val_loss: 0.6309 - val_acc: 0.8114\n",
            "Epoch 117/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7097 - acc: 0.7740Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 140us/sample - loss: 0.5841 - acc: 0.8116\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7098 - acc: 0.7740 - val_loss: 0.5966 - val_acc: 0.8116\n",
            "Epoch 118/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7066 - acc: 0.7782Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 150us/sample - loss: 0.7069 - acc: 0.7627\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7068 - acc: 0.7781 - val_loss: 0.7555 - val_acc: 0.7627\n",
            "Epoch 119/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.7766Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.5663 - acc: 0.8026\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7120 - acc: 0.7766 - val_loss: 0.6278 - val_acc: 0.8026\n",
            "Epoch 120/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7128 - acc: 0.7762Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 156us/sample - loss: 0.5932 - acc: 0.8287\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7125 - acc: 0.7763 - val_loss: 0.5591 - val_acc: 0.8287\n",
            "Epoch 121/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7050 - acc: 0.7785Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.5687 - acc: 0.8188\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7052 - acc: 0.7785 - val_loss: 0.5873 - val_acc: 0.8188\n",
            "Epoch 122/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7038 - acc: 0.7778Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 140us/sample - loss: 0.7146 - acc: 0.7861\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7038 - acc: 0.7779 - val_loss: 0.7653 - val_acc: 0.7861\n",
            "Epoch 123/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7049 - acc: 0.7791Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.8215 - acc: 0.7860\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7052 - acc: 0.7789 - val_loss: 0.6877 - val_acc: 0.7860\n",
            "Epoch 124/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7120 - acc: 0.7756Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.6267 - acc: 0.8081\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 0.7118 - acc: 0.7757 - val_loss: 0.5971 - val_acc: 0.8081\n",
            "Epoch 125/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.7769Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.6647 - acc: 0.8033\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7090 - acc: 0.7769 - val_loss: 0.6023 - val_acc: 0.8033\n",
            "Epoch 126/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7038 - acc: 0.7766Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.6807 - acc: 0.7886\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7038 - acc: 0.7766 - val_loss: 0.6629 - val_acc: 0.7886\n",
            "Epoch 127/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7066 - acc: 0.7776Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6629 - acc: 0.8077\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7066 - acc: 0.7776 - val_loss: 0.5998 - val_acc: 0.8077\n",
            "Epoch 128/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7053 - acc: 0.7789Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.5926 - acc: 0.7988\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7053 - acc: 0.7788 - val_loss: 0.6234 - val_acc: 0.7988\n",
            "Epoch 129/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7765Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 141us/sample - loss: 0.6075 - acc: 0.8173\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7060 - acc: 0.7765 - val_loss: 0.5632 - val_acc: 0.8173\n",
            "Epoch 130/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7128 - acc: 0.7756Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 145us/sample - loss: 0.5524 - acc: 0.8262\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7128 - acc: 0.7756 - val_loss: 0.5477 - val_acc: 0.8262\n",
            "Epoch 131/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7062 - acc: 0.7765Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6138 - acc: 0.8266\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7061 - acc: 0.7766 - val_loss: 0.5549 - val_acc: 0.8266\n",
            "Epoch 132/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7072 - acc: 0.7776Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.6395 - acc: 0.7871\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 0.7074 - acc: 0.7776 - val_loss: 0.6700 - val_acc: 0.7871\n",
            "Epoch 133/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7032 - acc: 0.7776Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 157us/sample - loss: 0.6175 - acc: 0.8202\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7027 - acc: 0.7777 - val_loss: 0.5954 - val_acc: 0.8202\n",
            "Epoch 134/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7019 - acc: 0.7781Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.6150 - acc: 0.8180\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7018 - acc: 0.7782 - val_loss: 0.5922 - val_acc: 0.8180\n",
            "Epoch 135/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7069 - acc: 0.7759Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.6326 - acc: 0.8026\n",
            "1562/1562 [==============================] - 33s 21ms/step - loss: 0.7067 - acc: 0.7760 - val_loss: 0.6899 - val_acc: 0.8026\n",
            "Epoch 136/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6989 - acc: 0.7800Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.6883 - acc: 0.7914\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6991 - acc: 0.7799 - val_loss: 0.6492 - val_acc: 0.7914\n",
            "Epoch 137/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7031 - acc: 0.7789Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 149us/sample - loss: 0.5353 - acc: 0.8164\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7033 - acc: 0.7789 - val_loss: 0.5991 - val_acc: 0.8164\n",
            "Epoch 138/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7009 - acc: 0.7788Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 140us/sample - loss: 0.5833 - acc: 0.8163\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7009 - acc: 0.7788 - val_loss: 0.5834 - val_acc: 0.8163\n",
            "Epoch 139/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6945 - acc: 0.7784Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 150us/sample - loss: 0.6574 - acc: 0.8149\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6946 - acc: 0.7783 - val_loss: 0.5947 - val_acc: 0.8149\n",
            "Epoch 140/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7070 - acc: 0.7781Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.7226 - acc: 0.8053\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7069 - acc: 0.7781 - val_loss: 0.6256 - val_acc: 0.8053\n",
            "Epoch 141/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7074 - acc: 0.7776Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.5955 - acc: 0.8317\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7073 - acc: 0.7776 - val_loss: 0.5330 - val_acc: 0.8317\n",
            "Epoch 142/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7033 - acc: 0.7792Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 144us/sample - loss: 0.7165 - acc: 0.8057\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7031 - acc: 0.7792 - val_loss: 0.6009 - val_acc: 0.8057\n",
            "Epoch 143/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7098 - acc: 0.7772Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.6925 - acc: 0.7891\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7101 - acc: 0.7772 - val_loss: 0.6527 - val_acc: 0.7891\n",
            "Epoch 144/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7082 - acc: 0.7774Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.6555 - acc: 0.7650\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7080 - acc: 0.7775 - val_loss: 0.7538 - val_acc: 0.7650\n",
            "Epoch 145/150\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7745Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 146us/sample - loss: 0.5553 - acc: 0.8227\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7097 - acc: 0.7744 - val_loss: 0.5512 - val_acc: 0.8227\n",
            "Epoch 146/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6990 - acc: 0.7786Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 140us/sample - loss: 0.5318 - acc: 0.8194\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6988 - acc: 0.7786 - val_loss: 0.5790 - val_acc: 0.8194\n",
            "Epoch 147/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7021 - acc: 0.7775Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 142us/sample - loss: 0.6747 - acc: 0.8036\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7024 - acc: 0.7774 - val_loss: 0.6355 - val_acc: 0.8036\n",
            "Epoch 148/150\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6953 - acc: 0.7789Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 160us/sample - loss: 0.5967 - acc: 0.8180\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6952 - acc: 0.7789 - val_loss: 0.5740 - val_acc: 0.8180\n",
            "Epoch 149/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7011 - acc: 0.7785Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 148us/sample - loss: 0.6281 - acc: 0.8084\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7013 - acc: 0.7785 - val_loss: 0.6024 - val_acc: 0.8084\n",
            "Epoch 150/150\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.6981 - acc: 0.7796Epoch 1/150\n",
            "10000/1562 [================================================================================================================================================================================================] - 1s 138us/sample - loss: 0.5377 - acc: 0.8214\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6982 - acc: 0.7796 - val_loss: 0.5897 - val_acc: 0.8214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CMETKZJzL5D",
        "colab_type": "text"
      },
      "source": [
        "Model Evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1fNZE0c0lTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "caf5c9c4-500c-4754-d8d0-5840815f9939"
      },
      "source": [
        "scores=model.evaluate(x_test,y_test,batch_size=128,verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.4387 - acc: 0.8597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJL23Kv-6f85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7231ef6d-c896-4b08-847f-605802ebfec8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'],loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICKi49IIIiI67j+Js2K684yrWAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}